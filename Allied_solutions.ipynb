{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9916c021-0067-4b68-9b66-549ef93865a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version : 3.11.14\n",
      "Pandas version : 2.2.3\n",
      "NumPy version  : 2.1.2\n",
      "SQLAlchemy     : 2.0.36\n",
      "\n",
      "Current working directory:\n",
      "/home/parallels/projects/insurance-data-ingestion\n",
      "\n",
      "Checking for required directories...\n",
      "Created missing directory: data\n",
      "Created missing directory: scripts\n",
      "Created missing directory: config\n",
      "Created missing directory: logs\n",
      "\n",
      "✅ Environment verification complete.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Cell 1: Environment Setup & Verification\n",
    "# ==========================================================\n",
    "# Purpose:\n",
    "#   - Confirm that the working directory is correct\n",
    "#   - Import required libraries for the ETL pipeline\n",
    "#   - Verify version compatibility and environment readiness\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "import yaml\n",
    "\n",
    "# --- Display environment info ---\n",
    "print(\"Python version :\", sys.version.split()[0])\n",
    "print(\"Pandas version :\", pd.__version__)\n",
    "print(\"NumPy version  :\", np.__version__)\n",
    "print(\"SQLAlchemy     :\", sqlalchemy.__version__)\n",
    "\n",
    "# --- Verify working directory ---\n",
    "cwd = os.getcwd()\n",
    "print(\"\\nCurrent working directory:\")\n",
    "print(cwd)\n",
    "\n",
    "expected_dirs = [\"data\", \"scripts\", \"config\", \"logs\"]\n",
    "print(\"\\nChecking for required directories...\")\n",
    "for d in expected_dirs:\n",
    "    if not os.path.exists(d):\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "        print(f\"Created missing directory: {d}\")\n",
    "    else:\n",
    "        print(f\"✓ Found: {d}\")\n",
    "\n",
    "print(\"\\n✅ Environment verification complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a78f596b-41e4-4146-992a-cb3ce9359681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ YAML mapping configuration created → config/client_mapping.yml\n",
      "✅ Mock raw data created → data/raw/client_insurance_feed.csv\n",
      "\n",
      "Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_no</th>\n",
       "      <th>insured_nm</th>\n",
       "      <th>eff_dt</th>\n",
       "      <th>exp_dt</th>\n",
       "      <th>state_cd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1000</td>\n",
       "      <td>Customer 1</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P1001</td>\n",
       "      <td>Customer 2</td>\n",
       "      <td>2024-01-16</td>\n",
       "      <td>2025-01-15</td>\n",
       "      <td>ind.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P1002</td>\n",
       "      <td>Customer 3</td>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P1003</td>\n",
       "      <td>Customer 4</td>\n",
       "      <td>2024-02-15</td>\n",
       "      <td>2025-02-14</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P1004</td>\n",
       "      <td>Customer 5</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>ind.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  policy_no  insured_nm     eff_dt     exp_dt state_cd\n",
       "0     P1000  Customer 1 2024-01-01 2024-12-31         \n",
       "1     P1001  Customer 2 2024-01-16 2025-01-15     ind.\n",
       "2     P1002  Customer 3 2024-01-31 2025-01-30         \n",
       "3     P1003  Customer 4 2024-02-15 2025-02-14         \n",
       "4     P1004  Customer 5 2024-03-01 2025-03-01     ind."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Cell 2: Configuration and Mock Data Setup\n",
    "# ==========================================================\n",
    "# Purpose:\n",
    "#   - Generate sample YAML mapping for column renaming and normalization\n",
    "#   - Create synthetic raw insurance data to simulate a client feed\n",
    "#   - Verify files are saved for downstream transformation\n",
    "# ==========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# --- Ensure directories exist ---\n",
    "os.makedirs(\"config\", exist_ok=True)\n",
    "os.makedirs(\"data/raw\", exist_ok=True)\n",
    "\n",
    "# --- 1. Create YAML mapping configuration ------------------\n",
    "mapping = {\n",
    "    \"rename_columns\": {\n",
    "        \"policy_no\": \"policy_number\",\n",
    "        \"insured_nm\": \"insured_name\",\n",
    "        \"eff_dt\": \"effective_date\",\n",
    "        \"exp_dt\": \"expiration_date\",\n",
    "        \"state_cd\": \"state\"\n",
    "    },\n",
    "    \"date_fields\": [\"effective_date\", \"expiration_date\"],\n",
    "    \"normalize_fields\": {\n",
    "        \"state\": {\n",
    "            \"ind.\": \"IN\",\n",
    "            \"ill.\": \"IL\",\n",
    "            \"\": \"UNKNOWN\"\n",
    "        }\n",
    "    },\n",
    "    \"drop_duplicates\": True\n",
    "}\n",
    "\n",
    "with open(\"config/client_mapping.yml\", \"w\") as f:\n",
    "    yaml.dump(mapping, f, sort_keys=False)\n",
    "\n",
    "print(\"✅ YAML mapping configuration created → config/client_mapping.yml\")\n",
    "\n",
    "\n",
    "# --- 2. Create mock insurance data feed --------------------\n",
    "np.random.seed(42)\n",
    "n_records = 20\n",
    "\n",
    "data = {\n",
    "    \"policy_no\": [f\"P{1000+i}\" for i in range(n_records)],\n",
    "    \"insured_nm\": [f\"Customer {i}\" for i in range(1, n_records+1)],\n",
    "    \"eff_dt\": pd.date_range(\"2024-01-01\", periods=n_records, freq=\"15D\"),\n",
    "    \"exp_dt\": pd.date_range(\"2024-12-31\", periods=n_records, freq=\"15D\"),\n",
    "    \"state_cd\": np.random.choice([\"ind.\", \"ill.\", \"\"], size=n_records)\n",
    "}\n",
    "\n",
    "df_raw = pd.DataFrame(data)\n",
    "df_raw.to_csv(\"data/raw/client_insurance_feed.csv\", index=False)\n",
    "\n",
    "print(\"✅ Mock raw data created → data/raw/client_insurance_feed.csv\")\n",
    "print(\"\\nSample:\")\n",
    "display(df_raw.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7bbbdac-8668-44e4-a707-46b15450ff1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw data: 20 records, 5 columns\n",
      "Dropped 0 duplicate rows (if any)\n",
      "✅ Transformation complete → data/processed/client_insurance_feed_processed.csv\n",
      "\n",
      "Sample of transformed data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_number</th>\n",
       "      <th>insured_name</th>\n",
       "      <th>effective_date</th>\n",
       "      <th>expiration_date</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1000</td>\n",
       "      <td>Customer 1</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P1001</td>\n",
       "      <td>Customer 2</td>\n",
       "      <td>2024-01-16</td>\n",
       "      <td>2025-01-15</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P1002</td>\n",
       "      <td>Customer 3</td>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P1003</td>\n",
       "      <td>Customer 4</td>\n",
       "      <td>2024-02-15</td>\n",
       "      <td>2025-02-14</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P1004</td>\n",
       "      <td>Customer 5</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  policy_number insured_name effective_date expiration_date state\n",
       "0         P1000   Customer 1     2024-01-01      2024-12-31   nan\n",
       "1         P1001   Customer 2     2024-01-16      2025-01-15    IN\n",
       "2         P1002   Customer 3     2024-01-31      2025-01-30   nan\n",
       "3         P1003   Customer 4     2024-02-15      2025-02-14   nan\n",
       "4         P1004   Customer 5     2024-03-01      2025-03-01    IN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Cell 3: Data Transformation\n",
    "# ==========================================================\n",
    "# Purpose:\n",
    "#   - Apply ETL transformation rules using YAML configuration\n",
    "#   - Standardize column names, formats, and categories\n",
    "#   - Save cleaned file to data/processed/ for validation\n",
    "# ==========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# --- Directories ---\n",
    "RAW_FILE = \"data/raw/client_insurance_feed.csv\"\n",
    "CONFIG_FILE = \"config/client_mapping.yml\"\n",
    "PROCESSED_DIR = \"data/processed\"\n",
    "os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "# --- Load configuration ---\n",
    "with open(CONFIG_FILE, \"r\") as f:\n",
    "    mapping = yaml.safe_load(f)\n",
    "\n",
    "# --- Read raw file ---\n",
    "df = pd.read_csv(RAW_FILE)\n",
    "print(f\"Loaded raw data: {df.shape[0]} records, {df.shape[1]} columns\")\n",
    "\n",
    "# --- Apply transformations ---\n",
    "# 1. Rename columns\n",
    "df = df.rename(columns=mapping[\"rename_columns\"])\n",
    "\n",
    "# 2. Convert date fields\n",
    "for col in mapping.get(\"date_fields\", []):\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "# 3. Normalize categorical text\n",
    "for col, rule in mapping.get(\"normalize_fields\", {}).items():\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.strip().str.lower().replace(rule)\n",
    "\n",
    "# 4. Drop duplicates if specified\n",
    "if mapping.get(\"drop_duplicates\", True):\n",
    "    before = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    after = len(df)\n",
    "    print(f\"Dropped {before - after} duplicate rows (if any)\")\n",
    "\n",
    "# --- Save output ---\n",
    "output_path = os.path.join(PROCESSED_DIR, \"client_insurance_feed_processed.csv\")\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✅ Transformation complete → {output_path}\")\n",
    "print(\"\\nSample of transformed data:\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e7ebff3-1109-40e9-9371-84a4ef92dfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded processed file: 20 rows, 5 columns\n",
      "✅ All expected columns present.\n",
      "\n",
      "Missing value summary:\n",
      "state    9\n",
      "dtype: int64\n",
      "\n",
      "Data types:\n",
      "policy_number      object\n",
      "insured_name       object\n",
      "effective_date     object\n",
      "expiration_date    object\n",
      "state              object\n",
      "dtype: object\n",
      "✅ Record count OK: 20 rows.\n",
      "\n",
      "🎯 Validation PASSED — file ready for SQL import.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Cell 4: Validation & Quality Checks\n",
    "# ==========================================================\n",
    "# Purpose:\n",
    "#   - Verify processed data conforms to expected schema\n",
    "#   - Detect missing or invalid values\n",
    "#   - Summarize validation results for downstream import\n",
    "# ==========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "PROCESSED_FILE = \"data/processed/client_insurance_feed_processed.csv\"\n",
    "\n",
    "# --- Load processed data ---\n",
    "if not os.path.exists(PROCESSED_FILE):\n",
    "    raise FileNotFoundError(\"Processed file not found. Run Cell 3 first.\")\n",
    "\n",
    "df = pd.read_csv(PROCESSED_FILE)\n",
    "print(f\"Loaded processed file: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "\n",
    "# --- Define expected schema ---\n",
    "expected_columns = [\"policy_number\", \"insured_name\", \"effective_date\", \"expiration_date\", \"state\"]\n",
    "\n",
    "missing_cols = [c for c in expected_columns if c not in df.columns]\n",
    "extra_cols = [c for c in df.columns if c not in expected_columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"❌ Missing expected columns: {missing_cols}\")\n",
    "else:\n",
    "    print(\"✅ All expected columns present.\")\n",
    "\n",
    "if extra_cols:\n",
    "    print(f\"⚠️ Extra columns detected: {extra_cols}\")\n",
    "\n",
    "# --- Check for missing values ---\n",
    "missing_summary = df.isna().sum()\n",
    "print(\"\\nMissing value summary:\")\n",
    "print(missing_summary[missing_summary > 0] if missing_summary.any() else \"✅ No missing values found.\")\n",
    "\n",
    "# --- Check data types ---\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# --- Record count sanity check ---\n",
    "if len(df) == 0:\n",
    "    print(\"❌ Validation failed: no records present.\")\n",
    "else:\n",
    "    print(f\"✅ Record count OK: {len(df)} rows.\")\n",
    "\n",
    "# --- Summary flag ---\n",
    "if not missing_cols and len(df) > 0:\n",
    "    print(\"\\n🎯 Validation PASSED — file ready for SQL import.\")\n",
    "else:\n",
    "    print(\"\\n⚠️ Validation FAILED — investigate missing or invalid fields.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c906d344-7139-4c59-aa78-8c2bf0b5ea5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 20 records into 'insurance_policies' table.\n",
      "\n",
      "Sample of data loaded into SQL table:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_number</th>\n",
       "      <th>insured_name</th>\n",
       "      <th>effective_date</th>\n",
       "      <th>expiration_date</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P1000</td>\n",
       "      <td>Customer 1</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P1001</td>\n",
       "      <td>Customer 2</td>\n",
       "      <td>2024-01-16</td>\n",
       "      <td>2025-01-15</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P1002</td>\n",
       "      <td>Customer 3</td>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>2025-01-30</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P1003</td>\n",
       "      <td>Customer 4</td>\n",
       "      <td>2024-02-15</td>\n",
       "      <td>2025-02-14</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P1004</td>\n",
       "      <td>Customer 5</td>\n",
       "      <td>2024-03-01</td>\n",
       "      <td>2025-03-01</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  policy_number insured_name effective_date expiration_date state\n",
       "0         P1000   Customer 1     2024-01-01      2024-12-31  None\n",
       "1         P1001   Customer 2     2024-01-16      2025-01-15    IN\n",
       "2         P1002   Customer 3     2024-01-31      2025-01-30  None\n",
       "3         P1003   Customer 4     2024-02-15      2025-02-14  None\n",
       "4         P1004   Customer 5     2024-03-01      2025-03-01    IN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Table schema:\n",
      " - policy_number (TEXT)\n",
      " - insured_name (TEXT)\n",
      " - effective_date (TEXT)\n",
      " - expiration_date (TEXT)\n",
      " - state (TEXT)\n",
      "\n",
      "✅ SQL import simulation complete → data/outputs/insurance_data.db\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Cell 5: SQL Import Simulation\n",
    "# ==========================================================\n",
    "# Purpose:\n",
    "#   - Simulate loading validated insurance data into a SQL table\n",
    "#   - Demonstrate schema creation, data insertion, and verification\n",
    "# ==========================================================\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# --- Paths and setup ---\n",
    "os.makedirs(\"data/outputs\", exist_ok=True)\n",
    "DB_PATH = \"data/outputs/insurance_data.db\"\n",
    "TABLE_NAME = \"insurance_policies\"\n",
    "PROCESSED_FILE = \"data/processed/client_insurance_feed_processed.csv\"\n",
    "\n",
    "# --- Load processed data ---\n",
    "df = pd.read_csv(PROCESSED_FILE)\n",
    "\n",
    "# --- Create SQLite database connection ---\n",
    "engine = create_engine(f\"sqlite:///{DB_PATH}\", echo=False)\n",
    "\n",
    "# --- Write data to SQL table (replace if exists) ---\n",
    "df.to_sql(TABLE_NAME, con=engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "# --- Confirm row count ---\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(f\"SELECT COUNT(*) FROM {TABLE_NAME}\"))\n",
    "    row_count = result.scalar_one()\n",
    "    print(f\"✅ Loaded {row_count} records into '{TABLE_NAME}' table.\")\n",
    "\n",
    "# --- Preview data ---\n",
    "preview = pd.read_sql(f\"SELECT * FROM {TABLE_NAME} LIMIT 5;\", engine)\n",
    "print(\"\\nSample of data loaded into SQL table:\")\n",
    "display(preview)\n",
    "\n",
    "# --- Verify schema ---\n",
    "with engine.connect() as conn:\n",
    "    schema_info = conn.execute(text(f\"PRAGMA table_info({TABLE_NAME});\")).fetchall()\n",
    "\n",
    "print(\"\\nTable schema:\")\n",
    "for col in schema_info:\n",
    "    print(f\" - {col[1]} ({col[2]})\")\n",
    "\n",
    "print(\"\\n✅ SQL import simulation complete → data/outputs/insurance_data.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72f53dab-9315-4f5f-b63d-efe89c7896c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 PIPELINE SUMMARY\n",
      "------------------------------------------------------------\n",
      "timestamp           : 2025-10-26T15:19:39.837574\n",
      "raw_input           : data/raw/client_insurance_feed.csv\n",
      "processed_output    : data/processed/client_insurance_feed_processed.csv\n",
      "database_output     : data/outputs/insurance_data.db\n",
      "table_name          : insurance_policies\n",
      "records_processed   : 20\n",
      "status              : SUCCESS\n",
      "validation_passed   : True\n",
      "notes               : All pipeline stages completed without errors.\n",
      "------------------------------------------------------------\n",
      "✅ Log written to → logs/pipeline_run_20251026_151939.json\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# Cell 6: Logging & Summary Report\n",
    "# ==========================================================\n",
    "# Purpose:\n",
    "#   - Provide end-to-end summary of ETL pipeline run\n",
    "#   - Write timestamped log to /logs directory\n",
    "# ==========================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Paths ---\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "LOG_PATH = f\"logs/pipeline_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "# --- Gather stats ---\n",
    "summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"raw_input\": \"data/raw/client_insurance_feed.csv\",\n",
    "    \"processed_output\": \"data/processed/client_insurance_feed_processed.csv\",\n",
    "    \"database_output\": \"data/outputs/insurance_data.db\",\n",
    "    \"table_name\": \"insurance_policies\",\n",
    "    \"records_processed\": len(df),\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"validation_passed\": True,\n",
    "    \"notes\": \"All pipeline stages completed without errors.\"\n",
    "}\n",
    "\n",
    "# --- Write log ---\n",
    "with open(LOG_PATH, \"w\") as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "# --- Display summary ---\n",
    "print(\"📋 PIPELINE SUMMARY\")\n",
    "print(\"-\" * 60)\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"✅ Log written to → {LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383928d1-cebc-4c9f-8688-69a83872fc36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
